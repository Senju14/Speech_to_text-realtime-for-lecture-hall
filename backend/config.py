# backend/config.py

# Audio Settings
SAMPLE_RATE = 16000
CHANNELS = 1
CHUNK_DURATION_MS = 500 # 0.5s chunks
CHUNK_SIZE = int(SAMPLE_RATE * CHUNK_DURATION_MS / 1000)

# VAD Settings (Adaptive Energy Based)
# L∆∞u √Ω: Code VADManager m·ªõi t·ª± ƒë·ªông th√≠ch ·ª©ng, nh∆∞ng ta v·∫´n gi·ªØ threshold c∆° s·ªü ·ªü ƒë√¢y
VAD_ENABLED = True
VAD_THRESHOLD = 0.015  # Ng∆∞·ª°ng RMS (Th·∫•p v√¨ l√† nƒÉng l∆∞·ª£ng) - C≈© l√† 0.5 (Sai)
VAD_MIN_SPEECH_MS = 250
VAD_MIN_SILENCE_MS = 500

# Audio Preprocessing
NOISE_REDUCE_ENABLED = False   # üëá QUAN TR·ªåNG: T·∫Øt ƒëi ƒë·ªÉ gi·∫£m ƒë·ªô tr·ªÖ (Frontend ƒë√£ lo r·ªìi)
NOISE_REDUCE_PROP_DECREASE = 0.5
HIGHPASS_ENABLED = True        # L·ªçc ti·∫øng √π (80Hz)
HIGHPASS_CUTOFF_HZ = 80
NORMALIZE_ENABLED = True       # Ch·ªëng v·ª° ti·∫øng (Soft Clipping)

# Local Agreement (Ch·ªëng gi·∫≠t)
LOCAL_AGREEMENT_N = 2      # C·∫ßn 2 l·∫ßn gi·ªëng nhau m·ªõi ch·ªët
BUFFER_TRIMMING_SEC = 15   # Gi·ªõi h·∫°n buffer t·ªïng
MIN_CHUNK_SIZE_SEC = 1.0

# Whisper E2E Model (Transcription + Translation)
WHISPER_MODEL = "openai/whisper-large-v3"  # E2E: supports both transcribe and translate
WHISPER_DEVICE = "cuda"
WHISPER_LANGUAGE = "vi"

# Server
WS_HOST = "0.0.0.0"
WS_PORT = 8000

# Modal Cloud Config
MODAL_APP_NAME = "asr-thesis"
MODAL_GPU = "A10G"      # GPU m·∫°nh, VRAM 24GB
MODAL_MEMORY = 16384    # 16GB RAM
MODAL_TIMEOUT = 600     # 10 ph√∫t timeout cho request
MODAL_CONTAINER_IDLE_TIMEOUT = 120 # 2 ph√∫t kh√¥ng d√πng th√¨ t·∫Øt container cho ƒë·ª° t·ªën ti·ªÅn

# Logging
LOG_LEVEL = "INFO"
LOG_TRANSCRIPTIONS = True
LOG_TIMING = True


SILENCE_LIMIT = 0.6
MAX_SEGMENT_SEC = 6.0
OVERLAP_SEC = 0.4
MIN_DECODE_SEC = 1.2