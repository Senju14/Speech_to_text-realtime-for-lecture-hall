================================================================================
            REAL-TIME SPEECH-TO-TEXT FOR LECTURE HALLS
                    Architecture Overview
================================================================================


1. FOLDER STRUCTURE
────────────────────────────────────────────────────────────────────────────────

ASR_TEMP/
├── main.py                          # Modal App entry (deploy/serve)
├── requirements.txt
├── pyproject.toml
├── architecture.txt                 # ← This file
├── .env / .env.example
│
├── src/                             # Backend source code
│   ├── __init__.py
│   │
│   ├── config/
│   │   └── settings.py              # All runtime config
│   │
│   ├── asr/
│   │   └── whisperx_asr.py          # WhisperX wrapper (transcribe + align)
│   │
│   ├── vad/
│   │   └── silero_vad.py            # Silero VAD (streaming speech detection)
│   │
│   ├── postprocess/
│   │   └── bartpho_corrector.py     # BARTpho syllable correction (LoRA)
│   │
│   ├── translation/
│   │   └── nllb_translator.py       # NLLB-200 (vi → en)
│   │
│   ├── llm/
│   │   └── groq_service.py          # Groq API (context priming + summary)
│   │
│   ├── session/
│   │   ├── handler.py               # ASRService + ASRSession
│   │   └── filters.py               # Hallucination filter
│   │
│   ├── api/
│   │   ├── routes.py                # HTTP REST endpoints
│   │   └── websocket.py             # WebSocket handler
│   │
│   └── utils/
│       ├── audio.py                 # PCM decode (base64 & binary)
│       └── torch_patch.py           # TF32, torch.load patch, warning filters
│
├── frontend/                        # Static web UI
│   ├── index.html                   # Main transcription page
│   ├── recordings.html              # Saved recordings viewer
│   ├── style.css
│   └── js/
│       ├── main.js                  # App init, event wiring
│       ├── audio.js                 # AudioWorklet capture (16kHz Int16)
│       ├── socket.js                # WebSocket client
│       ├── ui.js                    # DOM updates, transcript display
│       ├── utils.js                 # Helpers, toast system
│       ├── export.js                # Export transcript (TXT/SRT/JSON)
│       ├── recorder.worklet.js      # AudioWorkletProcessor
│       └── recordings.js            # Recordings page logic
│
└── test/
    ├── compare_models.py            # Modal benchmark (model comparison)
    ├── streaming_eval.py            # Streaming latency evaluation
    └── generate_tables.py           # LaTeX table generator


2. ARCHITECTURE DIAGRAM
────────────────────────────────────────────────────────────────────────────────

┌──────────────────────────────────────────────────────────────────────────┐
│                          CLIENT (Browser)                                │
│                                                                          │
│  AudioWorklet → 16kHz Int16 PCM → 250ms chunks → Binary WebSocket       │
└───────────────────────────────┬──────────────────────────────────────────┘
                                │
                     Binary WS (raw bytes, 33% smaller than base64)
                                │
                                ▼
┌──────────────────────────────────────────────────────────────────────────┐
│                     MODAL CONTAINER (A100 GPU, 24GB)                     │
│                                                                          │
│  ┌────────────────────────────────────────────────────────────────────┐  │
│  │                     FastAPI Application                            │  │
│  │                                                                    │  │
│  │  HTTP:  /api/status, /api/health                                   │  │
│  │         /api/expand-keywords, /api/summarize                       │  │
│  │  WS:   /ws/transcribe                                              │  │
│  └────────────────────────────────────────────────────────────────────┘  │
│                                │                                         │
│                                ▼                                         │
│  ┌────────────────────────────────────────────────────────────────────┐  │
│  │                     ASRSession (per client)                        │  │
│  │                                                                    │  │
│  │  Audio Buffer → VAD → Finalize → ASR → Post-Process → Translate   │  │
│  │                                                                    │  │
│  │  ┌──────────┐  ┌─────────┐  ┌──────────┐  ┌────────┐  ┌───────┐  │  │
│  │  │ Decode   │→ │ Silero  │→ │WhisperX  │→ │BARTpho │→ │ NLLB  │  │  │
│  │  │ Int16→f32│  │  VAD    │  │ large-v3 │  │Correct │  │vi→en  │  │  │
│  │  │ Buffer   │  │         │  │          │  │        │  │       │  │  │
│  │  └──────────┘  └─────────┘  └──────────┘  └────────┘  └───────┘  │  │
│  │                     │              │            │           │      │  │
│  │              speech/silence   transcribe    fix errors  translate  │  │
│  │              boundary          + align     (syllable)   (async)   │  │
│  └────────────────────────────────────────────────────────────────────┘  │
│                                                                          │
│  ┌────────────────────────────────────────────────────────────────────┐  │
│  │                     Shared Services (ASRService)                   │  │
│  │                                                                    │  │
│  │  • WhisperXASR       — Shared WhisperX model instance              │  │
│  │  • BARTphoCorrector  — Shared BARTpho + LoRA adapter               │  │
│  │  • NLLBTranslator    — Shared NLLB translation model               │  │
│  │  • GroqService       — Groq API client (keywords + summary)        │  │
│  └────────────────────────────────────────────────────────────────────┘  │
│                                                                          │
│  ┌────────────────────────────────────────────────────────────────────┐  │
│  │                     Model Cache (/cache volume)                    │  │
│  │                                                                    │  │
│  │  WhisperX large-v3  │  Pyannote VAD  │  wav2vec2-vi (alignment)   │  │
│  │  BARTpho + LoRA     │  NLLB-600M     │  Silero VAD                │  │
│  └────────────────────────────────────────────────────────────────────┘  │
└──────────────────────────────────────────────────────────────────────────┘


3. PROCESSING PIPELINE (per audio segment)
────────────────────────────────────────────────────────────────────────────────

  Browser                         Server
  ───────                         ──────
  Mic 16kHz Int16  ──250ms──→  1. Decode binary → float32
                                2. Append to buffer
                                3. Silero VAD: speech probability
                                4. Finalize when:
                                   • silence > 0.3s AND buffer > 0.3s
                                   • OR buffer > 4.0s (force)
                                   • OR client sends "stop"

                                5. WhisperX transcribe (batch_size=1, skip_align=True)
                                   + initial_prompt from Groq context priming
                                6. Hallucination filter (pattern matching)
                                7. BARTpho syllable correction (seq2seq LoRA)
                                8. Send source text immediately → client (is_final=False)
                                9. NLLB translate vi→en (async, non-blocking)
                               10. Send final result with translation → client (is_final=True)

  ←── JSON ────────────────────────────────────────────────────────────────
  {
    "type": "transcript",
    "segment_id": 1,
    "source": "xin chào các bạn",       ← corrected Vietnamese
    "target": "hello everyone",          ← English translation
    "is_final": true,
    "words": [...],                      ← word-level timestamps
    "timing": {
      "asr_ms": 150,                    ← WhisperX latency
      "pp_ms": 45,                      ← BARTpho correction latency
      "mt_ms": 80                       ← NLLB translation latency
    }
  }


4. MODEL INVENTORY
────────────────────────────────────────────────────────────────────────────────

  ┌───────────────────────┬──────────────────────────────────┬────────────┐
  │ Component             │ Model                            │ VRAM       │
  ├───────────────────────┼──────────────────────────────────┼────────────┤
  │ ASR                   │ WhisperX (large-v3)              │ ~6 GB      │
  │                       │ + faster-whisper CTranslate2     │            │
  │                       │ + Pyannote VAD (internal)        │            │
  ├───────────────────────┼──────────────────────────────────┼────────────┤
  │ Alignment             │ wav2vec2-vi (word timestamps)    │ ~1 GB      │
  ├───────────────────────┼──────────────────────────────────┼────────────┤
  │ Streaming VAD         │ Silero VAD v5 (torch.hub)        │ ~10 MB     │
  ├───────────────────────┼──────────────────────────────────┼────────────┤
  │ Post-processing       │ BARTpho-syllable + LoRA adapter  │ ~1.5 GB   │
  │                       │ 522H0134-.../bartpho-syllable-   │            │
  │                       │ correction (r=64, alpha=128)     │            │
  ├───────────────────────┼──────────────────────────────────┼────────────┤
  │ Translation           │ NLLB-200-distilled-600M          │ ~2.5 GB   │
  │                       │ vie_Latn → eng_Latn              │            │
  ├───────────────────────┼──────────────────────────────────┼────────────┤
  │ Context Priming       │ Groq API (llama-3.1-8b-instant)  │ 0 (API)   │
  │ + Auto Summary        │ Expand topic → keywords           │            │
  ├───────────────────────┼──────────────────────────────────┼────────────┤
  │ TOTAL (approx)        │                                  │ ~11 GB    │
  └───────────────────────┴──────────────────────────────────┴────────────┘


5. KEY FEATURES
────────────────────────────────────────────────────────────────────────────────

  a) Binary WebSocket
     - Frontend sends raw Int16 ArrayBuffer (not base64)
     - 33% bandwidth reduction vs base64 encoding

  b) Async Streaming
     - ASR result sent immediately (source text)
     - Translation runs async, sends update when ready
     - User sees Vietnamese text without waiting for English

  c) Context Priming (Groq LLM)
     - User enters lecture topic → Groq generates domain keywords
     - Keywords become WhisperX initial_prompt
     - Improves accuracy for technical terms

  d) Post-processing (BARTpho)
     - Seq2seq correction of Vietnamese syllable errors
     - LoRA fine-tuned on ASR error patterns
     - Runs between ASR and translation in pipeline

  e) Hallucination Filter
     - Pattern-based detection of Whisper hallucinations
     - Catches YouTube artifacts, music markers, sign-offs
     - Prevents noise from reaching the user

  f) Auto Summary (Groq LLM)
     - On session end (>2 min), generates lecture summary
     - Markdown: key points, technical terms, takeaways
     - Also available via manual trigger button


6. DEPLOYMENT
────────────────────────────────────────────────────────────────────────────────

  Platform:  Modal (serverless GPU)
  GPU:       NVIDIA A100 (24GB)
  Image:     Debian Slim + Python 3.11
  Volumes:   /cache (persistent model cache)
  Secrets:   groq-api-key, huggingface-secret

  Commands:
    modal deploy main.py          # Production deploy
    modal serve main.py           # Dev mode (hot-reload)

  URL: https://<user>--asr-thesis.modal.run


7. DEPENDENCIES
────────────────────────────────────────────────────────────────────────────────

  Core ML:         torch==2.5.1, torchaudio==2.5.1
  ASR:             whisperx (→ faster-whisper, pyannote, ctranslate2)
  Translation:     transformers, accelerate, sentencepiece
  Post-processing: peft (LoRA adapter loading)
  LLM:             groq
  Web:             fastapi, uvicorn, websockets, aiofiles
  Audio:           numpy, scipy, soundfile

================================================================================